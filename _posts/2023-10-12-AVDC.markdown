---
layout: post
title:  "Learning to Act from Actionless Video through Dense Correspondences"
date:   2023-10-12 00:00:00 +00:00
image: /images/avdc_teaser.png
categories: research
author: "Po-Chen Ko"
authors: "<strong>Po-Chen Ko</strong>, <a href=https://jiayuanm.com/>Jiayuan Mao</a>, <a href=https://yilundu.github.io/>Yilun Du</a>, <a href=https://shaohua0116.github.io/>Shao-Hua Sun</a>, <a href=https://cocosci.mit.edu/josh>Joshua B. Tenenbaum</a>"
venue: "ICLR 2024 (Spotlight)"
arxiv: https://arxiv.org/abs/2310.08576
code: https://github.com/flow-diffusion/AVDC
website: https://flow-diffusion.github.io/
paper: /pdfs/AVDC.pdf
video: https://github.com/flow-diffusion/flow-diffusion.github.io/assets/43379407/9aa380df-0ff7-4c41-af2d-d67d23c53e72
---
We developed a robot policy using images, which can be trained without action annotations. It's trained on RGB videos, effective for table-top tasks and navigation. Our framework also allows rapid modeling with just 4 GPUs in a day.