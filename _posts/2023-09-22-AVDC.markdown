---
layout: post
title:  "Learning to Act from Actionless Video through Dense Correspondences"
date:   2023-09-22 00:00:00 +00:00
image: /images/avatar.jpg
categories: research
author: "Po-Chen Ko"
authors: "<strong>Po-Chen Ko</strong>, <a href=https://jiayuanm.com/>Jiayuan Mao</a>, <a href=https://yilundu.github.io/>Yilun Du</a>, <a href=https://shaohua0116.github.io/>Shao-Hua Sun</a>, <a href=https://cocosci.mit.edu/josh>Joshua B. Tenenbaum</a>"
venue: "arXiv"
arxiv: https://arxiv.org/dummy
code: https://github.com/flow-diffusion
website: https://flow-diffusion.github.io/
---
We developed a video-based robot policy that operates without action annotations, using images to represent actions. This allows for training on RGB videos and deployment across various tasks. Our method proves effective for table-top actions and navigation. Additionally, we introduce an open-source framework for rapid video modeling, enabling training with only 4 GPUs in a day.